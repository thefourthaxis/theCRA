{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from statistics import *\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set file paths\n",
    "\n",
    "corp_path = 'C:/Project/corpus/'              #Path to corpus\n",
    "temp_path = 'C:/Project/temp/'                #Path for temporary files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_terms(vector,ngrams=(1,1)):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngrams,smooth_idf=True)\n",
    "    tf = vectorizer.fit_transform(vector)\n",
    "    idf = list(vectorizer.idf_)\n",
    "    all_features_idf = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "    return all_features_idf\n",
    "\n",
    "\n",
    "def terms_for_doc_table(file,text):\n",
    "    vect_list = [line.lower() for line in text]\n",
    "    # arguments vector name, ngram range\n",
    "    singles = extract_terms(vect_list,(1,1))\n",
    "    single_terms=[]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for key in singles:\n",
    "        if key not in stop :\n",
    "                if key.strip() not in ['',' ']:\n",
    "                    single_terms.append(key)\n",
    "    #print('File is', file)\n",
    "    #print('Number of single terms selected from corpus : ',len(single_terms))\n",
    "\n",
    "    \n",
    "    # arguments vector name, ngram range\n",
    "    corpus = extract_terms(vect_list,(2,4))\n",
    "    print('Number of features extracted from corpus : ',len(corpus))\n",
    "\n",
    "    #Retain only relevant terms which contain information about the data - remove very common words from the list.\n",
    "    all_rel_terms=[]\n",
    "    for key in corpus:\n",
    "        #if min_idf+(2*sd_idf) < corpus[key]  :\n",
    "                if key.strip() not in ['',' ']:\n",
    "                    all_rel_terms.append(key)\n",
    "    #print('Number of relevant features selected from corpus : ',len(all_rel_terms))\n",
    "    \n",
    "    return single_terms, all_rel_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(corp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a document table dataframe\n",
    "doc_table_list = []\n",
    "id = 0\n",
    "\n",
    "\n",
    "for file in file_list:\n",
    "    #Extract text from pdf files\n",
    "    # Known File read warning : PDF had issues during creation? \n",
    "    if '.pdf' in file :\n",
    "        id = id + 1\n",
    "        pdfFileObj = open(corp_path + file, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        #print(pdfReader.numPages)\n",
    "        pageObj = pdfReader.getPage(0)\n",
    "        text = pageObj.extractText()\n",
    "        pdfFileObj.close()\n",
    "        textlines = text.split(\"\\n\")\n",
    "        singles, all_terms = terms_for_doc_table(file,textlines)\n",
    "        doc_table_list.append([id,file, singles, all_terms])\n",
    "       \n",
    "    if '.txt' in file :\n",
    "        id = id + 1\n",
    "        textfile = open(corp_path + file, 'r')\n",
    "        text = textfile.read()\n",
    "        textlines = text.split(\"\\n\")\n",
    "        textfile.close()\n",
    "        singles, all_terms = terms_for_doc_table(file,textlines)\n",
    "        doc_table_list.append([id,file, singles, all_terms])\n",
    "        \n",
    "       \n",
    "      \n",
    "    #more lines here for other file types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_table = pd.DataFrame(doc_table_list, columns = ['ID', 'file_name', 'single_terms', 'phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_table.to_pickle(temp_path + \"CRA_doc_table.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
