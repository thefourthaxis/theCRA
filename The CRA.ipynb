{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Some technical details first. Let's import the python packages we would need.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import shutil\n",
    "from shutil import copyfile, copy\n",
    "from sys import exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set some paths up\n",
    "temp_path = 'C:/Project/temp/'                  #Path for temporary files created\n",
    "corpus_path = 'C:/Project/corpus/'              #Path to corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interaction with the User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The search query -  It comes from the user. Go ahead, type your search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell, type in your search query and then press Enter.\n",
    "the_query = input(\"Hi, what would you like to search for today? :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter a path for your results\n",
    "print(\"Great! Where would you like the results to be stored?\")\n",
    "pathexists = 0\n",
    "while pathexists == 0:\n",
    "    cache = input(\"Enter a valid path: \")\n",
    "    if os.path.isdir(cache) == True:\n",
    "        print('Your results will be stored in',cache)\n",
    "        pathexists = 1\n",
    "        break\n",
    "    else:\n",
    "        print(\"The path doesn't exist. Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question text will now be passed through the NL processor to ascertain what's being asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_proc(the_query):\n",
    "    spacy_nlp = spacy.load('en_core_web_sm')\n",
    "    doc = spacy_nlp(the_query)\n",
    "    relevant_pos = []\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for token in doc:\n",
    "        #print(token.text, token.pos_, token.tag_, token.dep_,token.shape_)\n",
    "        if token.pos_ in ['NOUN','VERB','ADJ'] and token.text not in stop:\n",
    "            relevant_pos.append([token.text,token.pos_])\n",
    "    return doc, relevant_pos\n",
    "\n",
    "def search_term(search_list, table_name, col1, col2):\n",
    "    files=[]\n",
    "    for search_term in search_list:\n",
    "        for file, rec in zip(table_name[col1],table_name[col2]):\n",
    "            for item in rec:\n",
    "                if search_term.lower() in item.strip():\n",
    "                    files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacyd_query,get_synonyms = nlp_proc(the_query)\n",
    "get_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [ent.text for ent in spacyd_query.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get synonyms for relevant search terms from the query\n",
    "synonyms={}\n",
    "lim = 3      #increase this limit if not enough matches in the first try\n",
    "for item in get_synonyms:\n",
    "    tempsyn =[item[0]]\n",
    "    if item[1] in 'NOUN':\n",
    "        POS_eq = wn.synsets(item[0],pos=wn.NOUN)\n",
    "    elif item[1] in 'VERB':\n",
    "        POS_eq = wn.synsets(item[0],pos=wn.VERB)\n",
    "    elif item[1] in 'ADJ':\n",
    "        POS_eq = wn.synsets(item[0],pos=wn.ADJ)\n",
    "    #entailments can be appended as well - see https://blog.xrds.acm.org/2017/07/power-wordnet-use-python/\n",
    "    for syn in POS_eq [:lim]: \n",
    "        for l in syn.lemmas():                                   #same\n",
    "            tempsyn.append(l.name()) \n",
    "        for h in syn.hypernyms():                                #similar\n",
    "            for i in h.lemmas():\n",
    "                tempsyn.append(i.name())\n",
    "        for e in syn.entailments():                              #implications\n",
    "            for j in e.lemmas():\n",
    "                tempsyn.append(j.name())                \n",
    "        for ph in syn.part_holonyms():                           #higher context in the hierarchy\n",
    "            for k in ph.lemmas():\n",
    "                tempsyn.append(k.name())                                \n",
    "        for pm in syn.part_holonyms():                           #lower context in the hierarchy\n",
    "            for m in pm.lemmas():\n",
    "                tempsyn.append(m.name())\n",
    "                \n",
    "    synonyms[item[0]]=list(set(tempsyn))\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import document table\n",
    "doc_table = pd.read_pickle(temp_path + \"CRA_doc_table.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search entities first\n",
    "docs_with_entities=[]\n",
    "if len(entities) > 0:\n",
    "    files = search_term(entities,doc_table,'ID','single_terms')\n",
    "    if len(files) > 0:\n",
    "        docs_with_entities=list(set(files))\n",
    "docs_with_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search other terms and get counts etc.\n",
    "docs_with_terms_and_synonyms = []\n",
    "for list_item in get_synonyms:\n",
    "    temp_list = []\n",
    "    files = search_term(synonyms[list_item[0]],doc_table,'ID','single_terms')\n",
    "    print(files)\n",
    "    if len(files) > 0:\n",
    "        if docs_with_terms_and_synonyms == []:\n",
    "            docs_with_terms_and_synonyms = files\n",
    "        else:\n",
    "            intersection = list(set(docs_with_terms_and_synonyms) & set(files))\n",
    "            if len(intersection) > 0:\n",
    "                docs_with_terms_and_synonyms = intersection\n",
    "status = ''\n",
    "if len(docs_with_terms_and_synonyms) > 0 and len(docs_with_entities) > 0:\n",
    "    intersection = list(set(docs_with_terms_and_synonyms) & set(docs_with_entities))\n",
    "    if len(intersection) > 0:\n",
    "        all_docs = intersection\n",
    "    else:\n",
    "        status = 'PARTIAL'\n",
    "        if len(docs_with_terms_and_synonyms) > len(docs_with_entities):\n",
    "            all_docs = docs_with_terms_and_synonyms\n",
    "        else:\n",
    "            all_docs = docs_with_entities\n",
    "        \n",
    "else:\n",
    "    status = 'PARTIAL'\n",
    "    all_docs = list(set(docs_with_terms_and_synonyms) | set(docs_with_entities))\n",
    "if not len(get_synonyms) > 0 or not len(entities) > 0:\n",
    "    status = ''\n",
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_docs) > 0:\n",
    "    for filen in all_docs:\n",
    "        for file, name in zip(doc_table['ID'],doc_table['file_name']):\n",
    "            if file == filen:\n",
    "                copy_file = name\n",
    "                source = os.path.join(corpus_path, copy_file)\n",
    "                dest = cache\n",
    "                shutil.copy(source, dest)\n",
    "else:\n",
    "    status = 'NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello there, I am back!')\n",
    "print('Your wanted to search for : ',the_query)\n",
    "if status == 'NONE':\n",
    "    print('Unfortunately, my search did not return any results.')\n",
    "    print('This is a rare event! Or perhaps it was just a spelling error? Would you like to try again?')\n",
    "else:\n",
    "    if status == 'PARTIAL':\n",
    "        print('Only a partial search was successful. You may want to re-word your search query to get better results.')                 \n",
    "    print('You will find the documents in',cache)\n",
    "    print('Let me know how it goes!')\n",
    "    print('Some Information:')\n",
    "    print('The following terms were used for the search:', end = \" \")\n",
    "    if len(synonyms) > 0:\n",
    "        for item in get_synonyms: \n",
    "            print(synonyms[item[0]],end = \" \" )\n",
    "    if len(entities) > 0:\n",
    "        print('as well as these name entities:', entities)    \n",
    "    print('Number of documents returned:', len(all_docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
